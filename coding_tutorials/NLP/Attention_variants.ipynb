{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["5zwSSUFOyMzP","TTawjppOyPeR","hPU727m9yFdj","wKhbmgqCyI90","61mMDeCJ9HXT","mrkBJNms4zGP","_pbTThrz88uF"],"authorship_tag":"ABX9TyOYvvYI0TZCKQjV4eCUs35x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Attention was all we need appearently...**\n","**made by:**\n","* Anas Aldadi\n","\n","<img src=\"https://drive.google.com/uc?id=1hQHQ81Zw8CFK0wczsuxn053gpxUxvlod\" width=\"800\"/>\n","\n","This notebook attempts to explain and implement most influential attention mechanism variants to be a place of reference if you ever forgot how one of them works!\n","\n","\n","---\n","\n","It is important to note this notebook assumes you know the following.\n","\n","Prerequisites:\n","\n","* Basics of deep learning (from FFNs-CNNs-RNNs)\n","\n","* Transformers and their archeticture variants (Encoder-Decoder, Encoder-Only, Decoder-Only)\n","\n","* Transformers by pre-training approaches (Masked LMs, AutoRegressive, Conditional Transformers)\n","\n","Why? because this notebook is intended to explain the variants of the attention mechanism not the transformer archeticture & without knowing transformers you will struggle to find value in this notebook, also another focus of this notebook is to be a comprehensive reference for ppl to refresh their memory about the different kinds of attention mechanism.\n","\n","---\n","\n","\n","The notebook will start with:\n","* Introduction: Why Attention?(analogy and the problem it solves)\n","* The Core of Attention: (Q, K, V explained intuitively and mathematically)\n","\n","Attention variants that will be explained here:\n","\n","* [Bahdanau Attention (Additive Attention) (2014)](https://arxiv.org/abs/1409.0473) NEURAL MACHINE TRANSLATION\n","BY JOINTLY LEARNING TO ALIGN AND TRANSLATE\n","\n","* [Luong Attention (Multiplicative Attention) (2015)](https://arxiv.org/abs/1508.04025) Effective Approaches to Attention-based Neural Machine Translation\n"," (aka soft attention)\n","\n","* [Hard Attention vs Soft Attention (2015)](https://arxiv.org/abs/1502.03044) Show, Attend and Tell: Neural Image Caption\n","Generation with Visual Attention\n","\n","* [Self Attention (2017)](https://arxiv.org/abs/1706.03762) Attention Is All You Need duh.\n","\n","1. Multi Head Attention (MHA)\n","\n","2. Scaled Dot-Product Attention (SDPA)\n","\n","3. Cross Attention (will be explained more in-depth in another notebook of multimodality)\n","\n","4. Causal/Masked Attention\n","\n","* [Sparse Attention / Sparse Transformers (2019)](https://arxiv.org/abs/1904.10509) Generating Long Sequences with Sparse Transformers\n","\n","* [Linformer (2020)](https://arxiv.org/abs/2006.04768) Linformer: Self-Attention with Linear Complexity\n","\n","* [Performer (2020)](https://arxiv.org/abs/2009.14794) Rethinking Attention with Performers\n","\n","* [Longformer (2020)](https://arxiv.org/abs/2004.05150) Longformer: The Long-Document Transformer\n","\n","* [BigBird (2020)](https://arxiv.org/abs/2007.14062) Big Bird: Transformers for Longer Sequences\n","\n","* [Multi Latent Attention (MLA) (2024)](https://arxiv.org/abs/2405.04434) DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model\n","\n","---\n","\n","\n","Not explained/implemented in this notebook but influential variants:\n","\n","* [FlashAttention (2022)](https://arxiv.org/abs/2205.14135) FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n","\n","    **Optimizations for MHA (for LLM Inference):**\n","\n","* [Multi-Query Attention (MQA) (2019)](https://arxiv.org/abs/1911.02150) Fast Transformer Decoding: One Write-Head is All You Need\n","\n","* [Grouped-Query Attention (GQA) (2023)](https://arxiv.org/abs/2305.13245) GQA: Training Generalized Multi-Query Transformer Models from\n","Multi-Head Checkpoints"],"metadata":{"id":"270t7K71oICx"}},{"cell_type":"markdown","source":["## Introduction"],"metadata":{"id":"3K-nwFc-v4RN"}},{"cell_type":"markdown","source":["Below is the abstract of the first attention paper. It beautifully addresses the problem and show the contribution neatly.\n","\n","<img src=\"https://drive.google.com/uc?id=1xNChrMndBghWdRdDlAvcZSrVhp6760mq\" width=\"800\"/>\n","\n","[Bahdanau Attention (Additive Attention) (2014)](https://arxiv.org/abs/1409.0473)\n","\n","the paper breifly explained:\n","\n"," they used a bidirectional RNN (LSTM units) (which was the SOTA at the time) and applied this new mechanism they proposed:\n","\n","<img src=\"https://drive.google.com/uc?id=1iVvmxozIaDHoUNvNBy8kn1pYKqTciJLX\" width=\"800\"/>\n","\n","And the goal was to allow the decoder to look back at all of the encoder's hidden states when generating each word in the target sequence.\n","\n","it showed really promising results! steady performence no matter how long the sequence is!\n","\n","<img src=\"https://drive.google.com/uc?id=1GoUbppumo3dFdNIEaq6HxxYjZoFGOTLO\" width=\"800\"/>\n","\n","---\n","\n","now i'll explain the attention mechanism in this paper with its notation then i'll map it to the QKV modern notation of the mechanism!"],"metadata":{"id":"QRL4OHsCr-JL"}},{"cell_type":"code","source":[],"metadata":{"id":"Qq3Be5YIv6Yy","executionInfo":{"status":"ok","timestamp":1748215970736,"user_tz":-180,"elapsed":16,"user":{"displayName":"A","userId":"14753180683882672476"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Attention variants"],"metadata":{"id":"UNwk92XAv6mi"}},{"cell_type":"markdown","source":["### Soft Attention"],"metadata":{"id":"5zwSSUFOyMzP"}},{"cell_type":"markdown","source":["* General concept, differentiability.\n","* Briefly contrast Additive (Bahdanau) vs. Multiplicative (Luong).\n","* Deep Dive: Scaled Dot-Product Attention (Crucial for Transformers)"],"metadata":{"id":"PJDQLZ0RyRh2"}},{"cell_type":"code","source":[],"metadata":{"id":"NUxVGXjQzdDR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Hard Attention vs Soft Attention"],"metadata":{"id":"TTawjppOyPeR"}},{"cell_type":"markdown","source":["Soft Attention: Differentiable, learns attention weights (e.g., Bahdanau).\n","\n","Hard Attention: Non-differentiable, uses sampling (e.g., REINFORCE), less common in practice due to training instability."],"metadata":{"id":"H0jXXeZ70YFS"}},{"cell_type":"code","source":[],"metadata":{"id":"4gMUe2_mzeeP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Self Attention"],"metadata":{"id":"hPU727m9yFdj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3ZWNezOZmVC"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils import F\n","\n","class Attention(nn.Module):\n","    def __init__(self, Q, K, V):\n","        self.Q = Q\n","        self.K = K\n","        self.V = V\n","\n","    def forward(self, d):\n","        attention = torch.softmax((self.Q * K.T)/torch.sqrt(d)) * V\n","        return attention"]},{"cell_type":"markdown","source":["#### Multihead Attention"],"metadata":{"id":"wKhbmgqCyI90"}},{"cell_type":"code","source":[],"metadata":{"id":"esbnc3mWyMI1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Scaled Dot-Product Attention"],"metadata":{"id":"61mMDeCJ9HXT"}},{"cell_type":"code","source":[],"metadata":{"id":"5VsK-FoS9ImG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Cross Attention"],"metadata":{"id":"mrkBJNms4zGP"}},{"cell_type":"markdown","source":["Key Idea: Used when attending from one modality to another (e.g., text-to-image, encoder-decoder models).\n","\n","Used In: Transformers, Diffusion Models (UNet + Cross Attention), Vision-Language models.\n","\n"],"metadata":{"id":"OhgT8T8W41dF"}},{"cell_type":"code","source":[],"metadata":{"id":"OrS59s-CyO_y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Causal/Masked Attention"],"metadata":{"id":"_pbTThrz88uF"}},{"cell_type":"markdown","source":["Where It’s Used\n","* GPT family (OpenAI)\n","\n","* Decoder-only Transformers\n","\n","* Autoregressive tasks like language modeling, image generation (e.g., DALL·E), code generation, etc."],"metadata":{"id":"gAz09oTq9KMe"}},{"cell_type":"code","source":[],"metadata":{"id":"tA5o0rsS9JEk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sparse Attention"],"metadata":{"id":"KjjG1Mkr5viy"}},{"cell_type":"code","source":[],"metadata":{"id":"TvU8WIcD5xiB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Linformer"],"metadata":{"id":"jzdwUNme9SfB"}},{"cell_type":"code","source":[],"metadata":{"id":"LcSz1rE9Ac8c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Performer"],"metadata":{"id":"F2FmomHrAehb"}},{"cell_type":"code","source":[],"metadata":{"id":"Erw8n_Q_AfpK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Longformer"],"metadata":{"id":"SFsrisSGAgkQ"}},{"cell_type":"code","source":[],"metadata":{"id":"E0Yh-NGzAgVh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BigBird"],"metadata":{"id":"xxF7S-i8Aj01"}},{"cell_type":"code","source":[],"metadata":{"id":"BifXTNHOAk2r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multi-Latent Attention (MLA)"],"metadata":{"id":"eMs7vRy5AnYU"}},{"cell_type":"code","source":[],"metadata":{"id":"J6aMRvLhAqg2"},"execution_count":null,"outputs":[]}]}